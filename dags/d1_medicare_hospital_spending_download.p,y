import warnings
from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
import requests
import pandas as pd
import logging
import boto3

# -------------------------------------------------------------------
# CONFIGURATION
# -------------------------------------------------------------------
warnings.filterwarnings('ignore')

CSV_URL = "https://data.cms.gov/provider-data/sites/default/files/resources/1f8cde9e222d5d49f88a894bcf7a8981_1736791547/Medicare_Hospital_Spending_by_Claim.csv"
S3_BUCKET = "ai-factory-bckt"
S3_PREFIX = "raw/medicare_hospital_spending"

s3 = boto3.client("s3")
logger = logging.getLogger(__name__)

# -------------------------------------------------------------------
# DAG DEFAULTS
# -------------------------------------------------------------------
default_args = {
    'owner': 'data-engineer',
    'start_date': datetime(2026, 1, 23),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(minutes=30),
}

# =============================================================================
# DAG DEFINITION
# =============================================================================
dag = DAG(
    'medicare_hospital_spending_download',
    default_args=default_args,
    schedule='@weekly',
    catchup=False,
    doc_md="""
    # Medicare Hospital Spending CSV DAG
    
    TÃ©lÃ©charge le CSV Medicare Hospital Spending et le stocke sur S3.
    """,
    tags=['medicare', 'csv', 's3'],
)

# -------------------------------------------------------------------
# TASK 1 : Download CSV
# -------------------------------------------------------------------
def download_csv(**context):
    local_path = "/tmp/Medicare_Hospital_Spending_by_Claim.csv"
    logger.info(f"ğŸ“¥ TÃ©lÃ©chargement depuis : {CSV_URL}")
    
    response = requests.get(CSV_URL, timeout=120)
    response.raise_for_status()
    
    with open(local_path, "wb") as f:
        f.write(response.content)
    
    file_size_mb = round(len(response.content) / (1024*1024), 2)
    logger.info(f"âœ… TÃ©lÃ©chargÃ© {file_size_mb} MB")
    
    context['task_instance'].xcom_push(key='local_path', value=local_path)
    context['task_instance'].xcom_push(key='file_size_mb', value=file_size_mb)

download_task = PythonOperator(
    task_id='download_csv',
    python_callable=download_csv,
    dag=dag
)

# -------------------------------------------------------------------
# TASK 2 : Validate CSV
# -------------------------------------------------------------------
def validate_csv(**context):
    local_path = context['task_instance'].xcom_pull(task_ids='download_csv', key='local_path')
    logger.info(f"ğŸ” Validation du CSV : {local_path}")
    
    df = pd.read_csv(local_path)
    row_count = df.shape[0]
    
    if row_count == 0:
        raise ValueError("âŒ CSV vide !")
    
    logger.info(f"âœ… Validation OK : {row_count:,} lignes, {df.shape[1]} colonnes")
    context['task_instance'].xcom_push(key='row_count', value=row_count)

validate_task = PythonOperator(
    task_id='validate_csv',
    python_callable=validate_csv,
    dag=dag
)

# -------------------------------------------------------------------
# TASK 3 : Upload to S3
# -------------------------------------------------------------------
def upload_csv_to_s3(**context):
    local_path = context['task_instance'].xcom_pull(task_ids='download_csv', key='local_path')
    row_count = context['task_instance'].xcom_pull(task_ids='validate_csv', key='row_count')
    file_size_mb = context['task_instance'].xcom_pull(task_ids='download_csv', key='file_size_mb')
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    s3_key_versioned = f"{S3_PREFIX}/Medicare_Hospital_Spending_{timestamp}.csv"
    s3_key_latest = f"{S3_PREFIX}/Medicare_Hospital_Spending_LATEST.csv"
    
    # Upload versionnÃ©
    s3.upload_file(local_path, S3_BUCKET, s3_key_versioned)
    logger.info(f"ğŸ“¤ Upload versionnÃ© sur s3://{S3_BUCKET}/{s3_key_versioned}")
    
    # Upload LATEST
    s3.upload_file(local_path, S3_BUCKET, s3_key_latest)
    logger.info(f"ğŸ“¤ Mis Ã  jour LATEST sur s3://{S3_BUCKET}/{s3_key_latest}")
    
    context['task_instance'].xcom_push(key='s3_key_versioned', value=s3_key_versioned)
    context['task_instance'].xcom_push(key='file_size_mb', value=file_size_mb)

upload_task = PythonOperator(
    task_id='upload_to_s3',
    python_callable=upload_csv_to_s3,
    dag=dag
)

# -------------------------------------------------------------------
# TASK 4 : Summary
# -------------------------------------------------------------------
def summarize_run(**context):
    s3_key = context['task_instance'].xcom_pull(task_ids='upload_to_s3', key='s3_key_versioned')
    row_count = context['task_instance'].xcom_pull(task_ids='validate_csv', key='row_count')
    file_size_mb = context['task_instance'].xcom_pull(task_ids='download_csv', key='file_size_mb')
    
    logger.info("="*70)
    logger.info("ğŸ“Š MEDICARE CSV DAG - SUMMARY")
    logger.info("="*70)
    logger.info(f"âœ… Taille fichier : {file_size_mb} MB")
    logger.info(f"âœ… Nombre de lignes : {row_count:,}")
    logger.info(f"âœ… S3 Path : s3://{S3_BUCKET}/{s3_key}")
    logger.info(f"âœ… Timestamp : {datetime.now().isoformat()}")
    logger.info("="*70)

summary_task = PythonOperator(
    task_id='summary',
    python_callable=summarize_run,
    dag=dag
)

# =============================================================================
# TASK 5 : Trigger DAG d2 (d2_open_payments_batch_by_year_download)
# =============================================================================
trigger_open_payments_batch_by_year = TriggerDagRunOperator(
    task_id='trigger_open_payments_batch_by_year',
    trigger_dag_id='open_payments_batch_by_year_download',
    wait_for_completion=False,
    dag=dag,
)

# -------------------------------------------------------------------
# DAG DEPENDENCIES
# -------------------------------------------------------------------
download_task >> validate_task >> upload_task >> summary_task >> trigger_open_payments_batch_by_year